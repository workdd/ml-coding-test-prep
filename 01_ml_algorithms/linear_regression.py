"""
ğŸ¯ ë¬¸ì œ: ì„ í˜• íšŒê·€ êµ¬í˜„

ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì„ í˜• íšŒê·€ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. í´ë˜ìŠ¤ ê¸°ë°˜ êµ¬í˜„ (LinearRegression)
2. fit() ë©”ì„œë“œë¡œ í•™ìŠµ
3. predict() ë©”ì„œë“œë¡œ ì˜ˆì¸¡
4. ê²½ì‚¬í•˜ê°•ë²• ì‚¬ìš©
5. ë¹„ìš©í•¨ìˆ˜(MSE) ê³„ì‚°
6. í•™ìŠµ ê³¼ì • ì‹œê°í™”

ìˆ˜í•™ì  ë°°ê²½:
- ê°€ì„¤: h(x) = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
- ë¹„ìš©í•¨ìˆ˜: J(Î¸) = (1/2m) * Î£(h(xâ½â±â¾) - yâ½â±â¾)Â²
- ê²½ì‚¬í•˜ê°•ë²•: Î¸â±¼ := Î¸â±¼ - Î± * âˆ‚J(Î¸)/âˆ‚Î¸â±¼
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List

class LinearRegression:
    def __init__(self, learning_rate: float = 0.01, max_iters: int = 1000, tol: float = 1e-6):
        """
        ì„ í˜• íšŒê·€ ì´ˆê¸°í™”
        
        Args:
            learning_rate: í•™ìŠµë¥ 
            max_iters: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            tol: ìˆ˜ë ´ í—ˆìš© ì˜¤ì°¨
        """
        # TODO: ì´ˆê¸°í™” êµ¬í˜„
        pass
    
    def _add_intercept(self, X: np.ndarray) -> np.ndarray:
        """
        ì ˆí¸(bias) í•­ì„ ìœ„í•œ 1 ì»¬ëŸ¼ ì¶”ê°€
        
        Args:
            X: ì…ë ¥ íŠ¹ì„± (n_samples, n_features)
            
        Returns:
            X_with_intercept: ì ˆí¸ í•­ì´ ì¶”ê°€ëœ íŠ¹ì„± (n_samples, n_features + 1)
        """
        # TODO: ì ˆí¸ í•­ ì¶”ê°€ êµ¬í˜„
        pass
    
    def _compute_cost(self, X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> float:
        """
        í‰ê·  ì œê³± ì˜¤ì°¨(MSE) ê³„ì‚°
        
        Args:
            X: ì…ë ¥ íŠ¹ì„± (ì ˆí¸ í•­ í¬í•¨)
            y: íƒ€ê²Ÿ ê°’
            theta: ëª¨ë¸ íŒŒë¼ë¯¸í„°
            
        Returns:
            cost: MSE ë¹„ìš©
        """
        # TODO: MSE ë¹„ìš©í•¨ìˆ˜ êµ¬í˜„
        # J(Î¸) = (1/2m) * Î£(h(xâ½â±â¾) - yâ½â±â¾)Â²
        pass
    
    def _compute_gradients(self, X: np.ndarray, y: np.ndarray, theta: np.ndarray) -> np.ndarray:
        """
        ê²½ì‚¬(gradient) ê³„ì‚°
        
        Args:
            X: ì…ë ¥ íŠ¹ì„± (ì ˆí¸ í•­ í¬í•¨)
            y: íƒ€ê²Ÿ ê°’
            theta: í˜„ì¬ íŒŒë¼ë¯¸í„°
            
        Returns:
            gradients: ê° íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ê²½ì‚¬
        """
        # TODO: ê²½ì‚¬ ê³„ì‚° êµ¬í˜„
        # âˆ‚J(Î¸)/âˆ‚Î¸â±¼ = (1/m) * Î£(h(xâ½â±â¾) - yâ½â±â¾) * xâ½â±â¾â±¼
        pass
    
    def fit(self, X: np.ndarray, y: np.ndarray) -> 'LinearRegression':
        """
        ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
        
        Args:
            X: í•™ìŠµ ë°ì´í„° íŠ¹ì„±
            y: í•™ìŠµ ë°ì´í„° íƒ€ê²Ÿ
            
        Returns:
            self: í•™ìŠµëœ ëª¨ë¸
        """
        # TODO: ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•œ í•™ìŠµ êµ¬í˜„
        # 1. ì ˆí¸ í•­ ì¶”ê°€
        # 2. íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”
        # 3. ê²½ì‚¬í•˜ê°•ë²• ë°˜ë³µ:
        #    - ë¹„ìš© ê³„ì‚°
        #    - ê²½ì‚¬ ê³„ì‚°
        #    - íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        #    - ìˆ˜ë ´ ì²´í¬
        pass
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        ì˜ˆì¸¡ ìˆ˜í–‰
        
        Args:
            X: ì˜ˆì¸¡í•  ë°ì´í„°
            
        Returns:
            predictions: ì˜ˆì¸¡ ê°’ë“¤
        """
        # TODO: ì˜ˆì¸¡ êµ¬í˜„
        pass
    
    def score(self, X: np.ndarray, y: np.ndarray) -> float:
        """
        RÂ² ì ìˆ˜ ê³„ì‚°
        
        Args:
            X: í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì„±
            y: í…ŒìŠ¤íŠ¸ ë°ì´í„° íƒ€ê²Ÿ
            
        Returns:
            r2_score: RÂ² ì ìˆ˜
        """
        # TODO: RÂ² ì ìˆ˜ êµ¬í˜„
        # RÂ² = 1 - (SS_res / SS_tot)
        pass
    
    def plot_cost_history(self):
        """
        ë¹„ìš© í•¨ìˆ˜ ë³€í™” ì‹œê°í™”
        """
        if not hasattr(self, 'cost_history'):
            print("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        plt.figure(figsize=(10, 6))
        plt.plot(self.cost_history)
        plt.title('Cost Function Over Iterations')
        plt.xlabel('Iterations')
        plt.ylabel('Cost (MSE)')
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def plot_predictions(self, X: np.ndarray, y: np.ndarray):
        """
        ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™” (1D íŠ¹ì„±ë§Œ)
        """
        if X.shape[1] != 1:
            print("1D íŠ¹ì„± ë°ì´í„°ë§Œ ì‹œê°í™” ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            return
        
        predictions = self.predict(X)
        
        plt.figure(figsize=(10, 6))
        plt.scatter(X, y, alpha=0.6, label='Actual')
        plt.plot(X, predictions, color='red', linewidth=2, label='Predicted')
        plt.title('Linear Regression Results')
        plt.xlabel('Feature')
        plt.ylabel('Target')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.show()

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    
    # 1D ì„ í˜• ë°ì´í„°
    X = np.random.randn(100, 1)
    y = 4 + 3 * X.ravel() + np.random.randn(100) * 0.5
    
    # ì„ í˜• íšŒê·€ ì‹¤í–‰
    lr = LinearRegression(learning_rate=0.01, max_iters=1000)
    lr.fit(X, y)
    
    # ê²°ê³¼ ì¶œë ¥
    predictions = lr.predict(X)
    r2_score = lr.score(X, y)
    
    print(f"í•™ìŠµëœ íŒŒë¼ë¯¸í„°: {lr.theta}")
    print(f"RÂ² ì ìˆ˜: {r2_score:.4f}")
    print(f"ìµœì¢… ë¹„ìš©: {lr.cost_history[-1]:.6f}")
    
    # ì‹œê°í™”
    lr.plot_cost_history()
    lr.plot_predictions(X, y)