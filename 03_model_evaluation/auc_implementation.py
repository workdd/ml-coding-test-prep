"""
ğŸ¯ ë¬¸ì œ: AUC (Area Under Curve) ê³„ì‚° êµ¬í˜„

ROC ê³¡ì„ ê³¼ AUCë¥¼ scratchë¶€í„° êµ¬í˜„í•˜ê³ , ë‹¤ì–‘í•œ ì„ê³„ê°’ì—ì„œì˜ ì„±ëŠ¥ì„ ë¶„ì„í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ROC ê³¡ì„  ê³„ì‚° í•¨ìˆ˜
2. AUC ìˆ˜ì¹˜ ê³„ì‚° (ì‚¬ë‹¤ë¦¬ê¼´ ê³µì‹ ì‚¬ìš©)
3. ìµœì  ì„ê³„ê°’ ì°¾ê¸°
4. PR ê³¡ì„  ë° AP ê³„ì‚°
5. ì‹œê°í™” ê¸°ëŠ¥

ìˆ˜í•™ì  ë°°ê²½:
- TPR (True Positive Rate) = TP / (TP + FN)
- FPR (False Positive Rate) = FP / (FP + TN)
- AUC = âˆ«â‚€Â¹ TPR(FPRâ»Â¹(x)) dx
- Precision = TP / (TP + FP)
- Recall = TP / (TP + FN)
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Dict
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

class AUCCalculator:
    def __init__(self):
        """
        AUC ê³„ì‚°ê¸° ì´ˆê¸°í™”
        """
        # TODO: ì´ˆê¸°í™” êµ¬í˜„
        pass
    
    def compute_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, int]:
        """
        í˜¼ë™ í–‰ë ¬ ê³„ì‚°
        
        Args:
            y_true: ì‹¤ì œ ë¼ë²¨
            y_pred: ì˜ˆì¸¡ ë¼ë²¨
            
        Returns:
            confusion_dict: TP, TN, FP, FN ë”•ì…”ë„ˆë¦¬
        """
        # TODO: í˜¼ë™ í–‰ë ¬ ê³„ì‚° êµ¬í˜„
        pass
    
    def compute_roc_curve(self, y_true: np.ndarray, y_scores: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        ROC ê³¡ì„  ê³„ì‚°
        
        Args:
            y_true: ì‹¤ì œ ë¼ë²¨ (0 ë˜ëŠ” 1)
            y_scores: ì˜ˆì¸¡ í™•ë¥  ë˜ëŠ” ì ìˆ˜
            
        Returns:
            fpr: False Positive Rate ë°°ì—´
            tpr: True Positive Rate ë°°ì—´
            thresholds: ì„ê³„ê°’ ë°°ì—´
        """
        # TODO: ROC ê³¡ì„  ê³„ì‚° êµ¬í˜„
        # 1. ì„ê³„ê°’ë“¤ì„ ì •ë ¬ëœ ì ìˆ˜ë¡œ ì„¤ì •
        # 2. ê° ì„ê³„ê°’ì—ì„œ TPR, FPR ê³„ì‚°
        # 3. (0,0)ê³¼ (1,1) í¬ì¸íŠ¸ ì¶”ê°€
        pass
    
    def compute_auc(self, fpr: np.ndarray, tpr: np.ndarray) -> float:
        """
        AUC ê³„ì‚° (ì‚¬ë‹¤ë¦¬ê¼´ ê³µì‹ ì‚¬ìš©)
        
        Args:
            fpr: False Positive Rate ë°°ì—´
            tpr: True Positive Rate ë°°ì—´
            
        Returns:
            auc: AUC ê°’
        """
        # TODO: ì‚¬ë‹¤ë¦¬ê¼´ ê³µì‹ìœ¼ë¡œ AUC ê³„ì‚°
        # AUC = Î£(x[i+1] - x[i]) * (y[i+1] + y[i]) / 2
        pass
    
    def find_optimal_threshold(self, y_true: np.ndarray, y_scores: np.ndarray, 
                             metric: str = 'youden') -> Tuple[float, float]:
        """
        ìµœì  ì„ê³„ê°’ ì°¾ê¸°
        
        Args:
            y_true: ì‹¤ì œ ë¼ë²¨
            y_scores: ì˜ˆì¸¡ ì ìˆ˜
            metric: ìµœì í™” ê¸°ì¤€ ('youden', 'f1', 'precision_recall')
            
        Returns:
            optimal_threshold: ìµœì  ì„ê³„ê°’
            optimal_score: í•´ë‹¹ ì„ê³„ê°’ì—ì„œì˜ ì ìˆ˜
        """
        # TODO: ìµœì  ì„ê³„ê°’ ì°¾ê¸° êµ¬í˜„
        # Youden's J statistic: J = TPR - FPR
        pass
    
    def compute_pr_curve(self, y_true: np.ndarray, y_scores: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Precision-Recall ê³¡ì„  ê³„ì‚°
        
        Args:
            y_true: ì‹¤ì œ ë¼ë²¨
            y_scores: ì˜ˆì¸¡ ì ìˆ˜
            
        Returns:
            precision: Precision ë°°ì—´
            recall: Recall ë°°ì—´
            thresholds: ì„ê³„ê°’ ë°°ì—´
        """
        # TODO: PR ê³¡ì„  ê³„ì‚° êµ¬í˜„
        pass
    
    def compute_average_precision(self, precision: np.ndarray, recall: np.ndarray) -> float:
        """
        Average Precision (AP) ê³„ì‚°
        
        Args:
            precision: Precision ë°°ì—´
            recall: Recall ë°°ì—´
            
        Returns:
            ap: Average Precision ê°’
        """
        # TODO: AP ê³„ì‚° êµ¬í˜„
        pass
    
    def plot_roc_curve(self, fpr: np.ndarray, tpr: np.ndarray, auc: float):
        """
        ROC ê³¡ì„  ì‹œê°í™”
        """
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve')
        plt.legend(loc="lower right")
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def plot_pr_curve(self, precision: np.ndarray, recall: np.ndarray, ap: float):
        """
        PR ê³¡ì„  ì‹œê°í™”
        """
        plt.figure(figsize=(8, 6))
        plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {ap:.3f})')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('Recall')
        plt.ylabel('Precision')
        plt.title('Precision-Recall Curve')
        plt.legend(loc="lower left")
        plt.grid(True, alpha=0.3)
        plt.show()
    
    def evaluate_model(self, y_true: np.ndarray, y_scores: np.ndarray) -> Dict:
        """
        ëª¨ë¸ ì¢…í•© í‰ê°€
        
        Args:
            y_true: ì‹¤ì œ ë¼ë²¨
            y_scores: ì˜ˆì¸¡ ì ìˆ˜
            
        Returns:
            evaluation: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # TODO: ì¢…í•© í‰ê°€ êµ¬í˜„
        pass

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    X, y = make_classification(n_samples=1000, n_features=20, n_redundant=0, 
                             n_informative=20, random_state=42, n_clusters_per_class=1)
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    
    # ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ í•™ìŠµ
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡ í™•ë¥  ê³„ì‚°
    y_scores = model.predict_proba(X_test)[:, 1]
    
    # AUC ê³„ì‚°ê¸° ì‚¬ìš©
    auc_calc = AUCCalculator()
    
    # ROC ê³¡ì„  ë° AUC ê³„ì‚°
    fpr, tpr, roc_thresholds = auc_calc.compute_roc_curve(y_test, y_scores)
    auc_score = auc_calc.compute_auc(fpr, tpr)
    
    # PR ê³¡ì„  ë° AP ê³„ì‚°
    precision, recall, pr_thresholds = auc_calc.compute_pr_curve(y_test, y_scores)
    ap_score = auc_calc.compute_average_precision(precision, recall)
    
    # ìµœì  ì„ê³„ê°’ ì°¾ê¸°
    optimal_threshold, optimal_score = auc_calc.find_optimal_threshold(y_test, y_scores)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"AUC Score: {auc_score:.4f}")
    print(f"Average Precision: {ap_score:.4f}")
    print(f"Optimal Threshold: {optimal_threshold:.4f}")
    print(f"Optimal Score: {optimal_score:.4f}")
    
    # ì‹œê°í™”
    auc_calc.plot_roc_curve(fpr, tpr, auc_score)
    auc_calc.plot_pr_curve(precision, recall, ap_score)
    
    # ì¢…í•© í‰ê°€
    evaluation = auc_calc.evaluate_model(y_test, y_scores)
    print("\nì¢…í•© í‰ê°€ ê²°ê³¼:")
    for key, value in evaluation.items():
        print(f"{key}: {value}")